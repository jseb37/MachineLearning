{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ArWK463kbhL",
    "outputId": "ad250ffe-29ed-4dc9-bf30-fe91ab10656c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mldzJdakbhS"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_b.csv')\n",
    "data=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsCrC2wckbhV",
    "outputId": "fff03fba-880e-4875-9bba-f05797f08d1d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-195.871045</td>\n",
       "      <td>-14843.084171</td>\n",
       "      <td>5.532140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1217.183964</td>\n",
       "      <td>-4068.124621</td>\n",
       "      <td>4.416082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.138451</td>\n",
       "      <td>4413.412028</td>\n",
       "      <td>0.425317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363.824242</td>\n",
       "      <td>15474.760647</td>\n",
       "      <td>1.094119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1            f2        f3    y\n",
       "0  -195.871045 -14843.084171  5.532140  1.0\n",
       "1 -1217.183964  -4068.124621  4.416082  1.0\n",
       "2     9.138451   4413.412028  0.425317  0.0\n",
       "3   363.824242  15474.760647  1.094119  0.0\n",
       "4  -768.812047  -7963.932192  1.870536  0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FI18joJ_kbhZ",
    "outputId": "22e420e9-4295-4307-a60f-1a528d07c81d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index   -0.014203\n",
       "f1       0.067172\n",
       "f2      -0.017944\n",
       "f3       0.839060\n",
       "y        1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u40oCVMikbhc",
    "outputId": "db6dce7e-7469-4aa5-8af3-1c08cd0f0081",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index       57.879185\n",
       "f1         488.195035\n",
       "f2       10403.417325\n",
       "f3           2.926662\n",
       "y            0.501255\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQIbNaHskbhe",
    "outputId": "f2298482-b1d5-47e0-f15c-31f4a753a9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "X=data[['f1','f2','f3']].values\n",
    "Y=data['y'].values\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUxp9-qEkbhh"
   },
   "source": [
    "# What if our features are with different variance \n",
    "\n",
    "<pre>\n",
    "* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n",
    "* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n",
    "\n",
    "> <b>Task1</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n",
    "\n",
    "> <b>Task2</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbMnsrxakbhi"
   },
   "source": [
    "<h3><font color='blue'> Make sure you write the observations for each task, why a particular feautre got more importance than others</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression(SGD Classifier With Logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-195.871045</td>\n",
       "      <td>-14843.084171</td>\n",
       "      <td>5.53214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1            f2       f3\n",
       "0 -195.871045 -14843.084171  5.53214"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['y'].values\n",
    "X = data.drop(['y'], axis=1)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.08, NNZs: 3, Bias: -0.001751, T: 200, Avg. loss: 2516.147588\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001551, T: 400, Avg. loss: 2621.694380\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.001850, T: 600, Avg. loss: 3285.222158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.003527, T: 800, Avg. loss: 3142.216822\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.004027, T: 1000, Avg. loss: 3009.886714\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.40, NNZs: 3, Bias: -0.003523, T: 1200, Avg. loss: 3032.001946\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 6 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.fit(X,y) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37170471, -1.34463853,  0.12669033]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.37170\n",
      "Feature: 1, Score: -1.34464\n",
      "Feature: 2, Score: 0.12669\n"
     ]
    }
   ],
   "source": [
    "#Negative feature importance value means that feature makes the loss go up.This might mean that your model is underfit (not enough iteration and it has not used the feature enough) or that the feature is not good and you can try removing it to improve final quality\n",
    "\n",
    "importance = clf_lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM(SGDClassifier with hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001600, T: 200, Avg. loss: 2634.084615\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.001100, T: 400, Avg. loss: 2593.136418\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000900, T: 600, Avg. loss: 3308.216351\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.002700, T: 800, Avg. loss: 3155.085896\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.93, NNZs: 3, Bias: -0.002800, T: 1000, Avg. loss: 3080.501847\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.002700, T: 1200, Avg. loss: 3011.887174\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.002200, T: 1400, Avg. loss: 3002.132514\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 7 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.38249\n",
      "Feature: 1, Score: -0.55765\n",
      "Feature: 2, Score: 0.15408\n"
     ]
    }
   ],
   "source": [
    "#Negative feature importance value means that feature makes the loss go up.This might mean that your model is underfit (not enough iteration and it has not used the feature enough) or that the feature is not good and you can try removing it to improve final quality\n",
    "\n",
    "importance = clf_svm.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations From Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As feature 2 has very high variance,it leads to negative feature importnace which makes loss go up after each epoch.This implies feature 2 is not good and we can try removing it or do a standardization on data.\n",
    "\n",
    "Feature 3 has variance close to zero which makes it a less important feature (they don’t meaningfully contribute to the model’s predictive capability)\n",
    "\n",
    "Feature 1 is having high importance compared to other two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2 - After Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression(SGD Classifier With Logloss) -After Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_std = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf_lr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 3, Bias: 0.000001, T: 200, Avg. loss: 0.691431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000002, T: 400, Avg. loss: 0.687922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000002, T: 600, Avg. loss: 0.684449\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000003, T: 800, Avg. loss: 0.681011\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 3, Bias: 0.000003, T: 1000, Avg. loss: 0.677608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000003, T: 1200, Avg. loss: 0.674240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.06, NNZs: 3, Bias: 0.000003, T: 1400, Avg. loss: 0.670905\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000003, T: 1600, Avg. loss: 0.667605\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000002, T: 1800, Avg. loss: 0.664338\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000002, T: 2000, Avg. loss: 0.661104\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000002, T: 2200, Avg. loss: 0.657903\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000001, T: 2400, Avg. loss: 0.654734\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2600, Avg. loss: 0.651598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2800, Avg. loss: 0.648493\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000001, T: 3000, Avg. loss: 0.645419\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000002, T: 3200, Avg. loss: 0.642377\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000002, T: 3400, Avg. loss: 0.639365\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000001, T: 3600, Avg. loss: 0.636383\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000001, T: 3800, Avg. loss: 0.633431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.16, NNZs: 3, Bias: 0.000001, T: 4000, Avg. loss: 0.630509\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000001, T: 4200, Avg. loss: 0.627616\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4400, Avg. loss: 0.624752\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4600, Avg. loss: 0.621917\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.19, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.619110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 5000, Avg. loss: 0.616331\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5200, Avg. loss: 0.613579\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5400, Avg. loss: 0.610855\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000001, T: 5600, Avg. loss: 0.608158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000002, T: 5800, Avg. loss: 0.605488\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000003, T: 6000, Avg. loss: 0.602845\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6200, Avg. loss: 0.600227\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.25, NNZs: 3, Bias: -0.000002, T: 6400, Avg. loss: 0.597635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6600, Avg. loss: 0.595069\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6800, Avg. loss: 0.592528\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.27, NNZs: 3, Bias: -0.000003, T: 7000, Avg. loss: 0.590011\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7200, Avg. loss: 0.587519\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7400, Avg. loss: 0.585052\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000004, T: 7600, Avg. loss: 0.582608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 7800, Avg. loss: 0.580189\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 8000, Avg. loss: 0.577793\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.31, NNZs: 3, Bias: -0.000007, T: 8200, Avg. loss: 0.575420\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.32, NNZs: 3, Bias: -0.000008, T: 8400, Avg. loss: 0.573071\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000009, T: 8600, Avg. loss: 0.570743\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000010, T: 8800, Avg. loss: 0.568439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000010, T: 9000, Avg. loss: 0.566156\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000010, T: 9200, Avg. loss: 0.563896\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000012, T: 9400, Avg. loss: 0.561657\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.36, NNZs: 3, Bias: -0.000012, T: 9600, Avg. loss: 0.559439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000013, T: 9800, Avg. loss: 0.557243\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000015, T: 10000, Avg. loss: 0.555067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.38, NNZs: 3, Bias: -0.000016, T: 10200, Avg. loss: 0.552912\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000017, T: 10400, Avg. loss: 0.550777\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000018, T: 10600, Avg. loss: 0.548663\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000020, T: 10800, Avg. loss: 0.546568\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000021, T: 11000, Avg. loss: 0.544493\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.41, NNZs: 3, Bias: -0.000023, T: 11200, Avg. loss: 0.542438\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000024, T: 11400, Avg. loss: 0.540402\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000025, T: 11600, Avg. loss: 0.538384\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.000027, T: 11800, Avg. loss: 0.536386\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000028, T: 12000, Avg. loss: 0.534406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000030, T: 12200, Avg. loss: 0.532444\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000030, T: 12400, Avg. loss: 0.530501\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000032, T: 12600, Avg. loss: 0.528575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.46, NNZs: 3, Bias: -0.000034, T: 12800, Avg. loss: 0.526668\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000035, T: 13000, Avg. loss: 0.524777\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000038, T: 13200, Avg. loss: 0.522904\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.000039, T: 13400, Avg. loss: 0.521048\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000042, T: 13600, Avg. loss: 0.519209\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000046, T: 13800, Avg. loss: 0.517387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000049, T: 14000, Avg. loss: 0.515581\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000054, T: 14200, Avg. loss: 0.513791\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000057, T: 14400, Avg. loss: 0.512018\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000060, T: 14600, Avg. loss: 0.510260\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000064, T: 14800, Avg. loss: 0.508518\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000067, T: 15000, Avg. loss: 0.506792\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000069, T: 15200, Avg. loss: 0.505081\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000072, T: 15400, Avg. loss: 0.503385\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000076, T: 15600, Avg. loss: 0.501705\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.55, NNZs: 3, Bias: -0.000080, T: 15800, Avg. loss: 0.500039\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000084, T: 16000, Avg. loss: 0.498387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000088, T: 16200, Avg. loss: 0.496751\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000091, T: 16400, Avg. loss: 0.495128\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000095, T: 16600, Avg. loss: 0.493520\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000100, T: 16800, Avg. loss: 0.491925\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000104, T: 17000, Avg. loss: 0.490345\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000108, T: 17200, Avg. loss: 0.488778\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000113, T: 17400, Avg. loss: 0.487225\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000118, T: 17600, Avg. loss: 0.485685\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000123, T: 17800, Avg. loss: 0.484158\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000128, T: 18000, Avg. loss: 0.482644\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000132, T: 18200, Avg. loss: 0.481144\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000137, T: 18400, Avg. loss: 0.479656\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000143, T: 18600, Avg. loss: 0.478180\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000147, T: 18800, Avg. loss: 0.476718\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000153, T: 19000, Avg. loss: 0.475267\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000158, T: 19200, Avg. loss: 0.473829\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000164, T: 19400, Avg. loss: 0.472402\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000170, T: 19600, Avg. loss: 0.470988\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 0.66, NNZs: 3, Bias: -0.000176, T: 19800, Avg. loss: 0.469585\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000182, T: 20000, Avg. loss: 0.468194\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000188, T: 20200, Avg. loss: 0.466815\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000194, T: 20400, Avg. loss: 0.465447\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000201, T: 20600, Avg. loss: 0.464090\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000207, T: 20800, Avg. loss: 0.462744\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000215, T: 21000, Avg. loss: 0.461410\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000222, T: 21200, Avg. loss: 0.460086\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000228, T: 21400, Avg. loss: 0.458773\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000235, T: 21600, Avg. loss: 0.457471\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000241, T: 21800, Avg. loss: 0.456179\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000249, T: 22000, Avg. loss: 0.454898\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000255, T: 22200, Avg. loss: 0.453627\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000262, T: 22400, Avg. loss: 0.452366\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000269, T: 22600, Avg. loss: 0.451115\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000277, T: 22800, Avg. loss: 0.449874\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000284, T: 23000, Avg. loss: 0.448643\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000292, T: 23200, Avg. loss: 0.447422\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000300, T: 23400, Avg. loss: 0.446210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000308, T: 23600, Avg. loss: 0.445008\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000316, T: 23800, Avg. loss: 0.443816\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000326, T: 24000, Avg. loss: 0.442632\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000334, T: 24200, Avg. loss: 0.441458\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000343, T: 24400, Avg. loss: 0.440293\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000352, T: 24600, Avg. loss: 0.439138\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000362, T: 24800, Avg. loss: 0.437991\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000370, T: 25000, Avg. loss: 0.436852\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000378, T: 25200, Avg. loss: 0.435723\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000388, T: 25400, Avg. loss: 0.434602\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000397, T: 25600, Avg. loss: 0.433490\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000407, T: 25800, Avg. loss: 0.432387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000417, T: 26000, Avg. loss: 0.431291\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000426, T: 26200, Avg. loss: 0.430204\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000436, T: 26400, Avg. loss: 0.429125\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000446, T: 26600, Avg. loss: 0.428055\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000457, T: 26800, Avg. loss: 0.426992\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000467, T: 27000, Avg. loss: 0.425937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000478, T: 27200, Avg. loss: 0.424891\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000490, T: 27400, Avg. loss: 0.423851\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000501, T: 27600, Avg. loss: 0.422820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000512, T: 27800, Avg. loss: 0.421796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000523, T: 28000, Avg. loss: 0.420780\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000534, T: 28200, Avg. loss: 0.419771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000546, T: 28400, Avg. loss: 0.418770\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000556, T: 28600, Avg. loss: 0.417776\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000568, T: 28800, Avg. loss: 0.416789\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000579, T: 29000, Avg. loss: 0.415809\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000591, T: 29200, Avg. loss: 0.414836\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000603, T: 29400, Avg. loss: 0.413871\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 147 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_std.fit(x,y) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.03851\n",
      "Feature: 1, Score: -0.00553\n",
      "Feature: 2, Score: 0.88963\n"
     ]
    }
   ],
   "source": [
    "#Negative feature importance value means that feature makes the loss go up.This might mean that your model is underfit (not enough iteration and it has not used the feature enough) or that the feature is not good and you can try removing it to improve final quality\n",
    "\n",
    "importance = clf_lr_std.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM(SGDClassifier with hinge)-After Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_std = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf_svm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000000, T: 200, Avg. loss: 0.993111\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.03, NNZs: 3, Bias: -0.000000, T: 400, Avg. loss: 0.978934\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000000, T: 600, Avg. loss: 0.964757\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 800, Avg. loss: 0.950580\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.08, NNZs: 3, Bias: -0.000000, T: 1000, Avg. loss: 0.936403\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000000, T: 1200, Avg. loss: 0.922226\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000000, T: 1400, Avg. loss: 0.908049\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000000, T: 1600, Avg. loss: 0.893873\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 1800, Avg. loss: 0.879696\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000000, T: 2000, Avg. loss: 0.865519\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.19, NNZs: 3, Bias: -0.000000, T: 2200, Avg. loss: 0.851342\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 2400, Avg. loss: 0.837165\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000000, T: 2600, Avg. loss: 0.822988\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000000, T: 2800, Avg. loss: 0.808812\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.25, NNZs: 3, Bias: 0.000000, T: 3000, Avg. loss: 0.794635\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.27, NNZs: 3, Bias: 0.000000, T: 3200, Avg. loss: 0.780458\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000000, T: 3400, Avg. loss: 0.766282\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.30, NNZs: 3, Bias: 0.000000, T: 3600, Avg. loss: 0.752105\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.32, NNZs: 3, Bias: 0.000000, T: 3800, Avg. loss: 0.737929\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000000, T: 4000, Avg. loss: 0.723752\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.35, NNZs: 3, Bias: 0.000000, T: 4200, Avg. loss: 0.709575\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.37, NNZs: 3, Bias: 0.000000, T: 4400, Avg. loss: 0.695399\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000000, T: 4600, Avg. loss: 0.681222\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.40, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.667046\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.42, NNZs: 3, Bias: 0.000000, T: 5000, Avg. loss: 0.652870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.44, NNZs: 3, Bias: 0.000000, T: 5200, Avg. loss: 0.638693\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.45, NNZs: 3, Bias: 0.000000, T: 5400, Avg. loss: 0.624517\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000000, T: 5600, Avg. loss: 0.610341\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000000, T: 5800, Avg. loss: 0.596164\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000000, T: 6000, Avg. loss: 0.581988\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000000, T: 6200, Avg. loss: 0.567812\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000000, T: 6400, Avg. loss: 0.553635\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000100, T: 6600, Avg. loss: 0.539559\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000400, T: 6800, Avg. loss: 0.525831\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000400, T: 7000, Avg. loss: 0.512822\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000300, T: 7200, Avg. loss: 0.500814\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.62, NNZs: 3, Bias: 0.000100, T: 7400, Avg. loss: 0.489867\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.63, NNZs: 3, Bias: 0.000500, T: 7600, Avg. loss: 0.479843\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.64, NNZs: 3, Bias: 0.000900, T: 7800, Avg. loss: 0.470023\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.66, NNZs: 3, Bias: 0.001200, T: 8000, Avg. loss: 0.460939\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.67, NNZs: 3, Bias: 0.001100, T: 8200, Avg. loss: 0.453123\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.68, NNZs: 3, Bias: 0.001000, T: 8400, Avg. loss: 0.446506\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.69, NNZs: 3, Bias: 0.001000, T: 8600, Avg. loss: 0.440244\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.70, NNZs: 3, Bias: 0.001000, T: 8800, Avg. loss: 0.434183\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.71, NNZs: 3, Bias: 0.001200, T: 9000, Avg. loss: 0.428457\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.72, NNZs: 3, Bias: 0.001400, T: 9200, Avg. loss: 0.422915\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.73, NNZs: 3, Bias: 0.001500, T: 9400, Avg. loss: 0.417423\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.74, NNZs: 3, Bias: 0.001800, T: 9600, Avg. loss: 0.412336\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.75, NNZs: 3, Bias: 0.001900, T: 9800, Avg. loss: 0.407610\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.76, NNZs: 3, Bias: 0.002200, T: 10000, Avg. loss: 0.403083\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.77, NNZs: 3, Bias: 0.002500, T: 10200, Avg. loss: 0.398796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.78, NNZs: 3, Bias: 0.002900, T: 10400, Avg. loss: 0.394692\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.79, NNZs: 3, Bias: 0.003400, T: 10600, Avg. loss: 0.390702\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.80, NNZs: 3, Bias: 0.004000, T: 10800, Avg. loss: 0.386766\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.004800, T: 11000, Avg. loss: 0.383045\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.005600, T: 11200, Avg. loss: 0.379523\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.82, NNZs: 3, Bias: 0.006400, T: 11400, Avg. loss: 0.376092\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.83, NNZs: 3, Bias: 0.006900, T: 11600, Avg. loss: 0.372853\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.84, NNZs: 3, Bias: 0.007400, T: 11800, Avg. loss: 0.369757\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.007900, T: 12000, Avg. loss: 0.366660\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.008500, T: 12200, Avg. loss: 0.363659\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.86, NNZs: 3, Bias: 0.008900, T: 12400, Avg. loss: 0.360918\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009300, T: 12600, Avg. loss: 0.358293\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009700, T: 12800, Avg. loss: 0.355741\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.88, NNZs: 3, Bias: 0.010200, T: 13000, Avg. loss: 0.353291\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010500, T: 13200, Avg. loss: 0.350947\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010800, T: 13400, Avg. loss: 0.348714\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.90, NNZs: 3, Bias: 0.011000, T: 13600, Avg. loss: 0.346524\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 13800, Avg. loss: 0.344410\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 14000, Avg. loss: 0.342392\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.92, NNZs: 3, Bias: 0.010900, T: 14200, Avg. loss: 0.340372\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010800, T: 14400, Avg. loss: 0.338400\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010600, T: 14600, Avg. loss: 0.336501\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 14800, Avg. loss: 0.334604\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 15000, Avg. loss: 0.332782\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.95, NNZs: 3, Bias: 0.010300, T: 15200, Avg. loss: 0.331037\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010200, T: 15400, Avg. loss: 0.329317\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010100, T: 15600, Avg. loss: 0.327597\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 15800, Avg. loss: 0.325932\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 16000, Avg. loss: 0.324369\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16200, Avg. loss: 0.322840\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16400, Avg. loss: 0.321310\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010000, T: 16600, Avg. loss: 0.319872\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010100, T: 16800, Avg. loss: 0.318513\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010300, T: 17000, Avg. loss: 0.317175\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010500, T: 17200, Avg. loss: 0.315862\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010700, T: 17400, Avg. loss: 0.314549\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010900, T: 17600, Avg. loss: 0.313237\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011100, T: 17800, Avg. loss: 0.311924\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011300, T: 18000, Avg. loss: 0.310612\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011500, T: 18200, Avg. loss: 0.309337\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011700, T: 18400, Avg. loss: 0.308166\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.011900, T: 18600, Avg. loss: 0.307057\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.012200, T: 18800, Avg. loss: 0.305980\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012500, T: 19000, Avg. loss: 0.304914\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012800, T: 19200, Avg. loss: 0.303880\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013100, T: 19400, Avg. loss: 0.302892\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013500, T: 19600, Avg. loss: 0.301927\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.013900, T: 19800, Avg. loss: 0.300979\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014100, T: 20000, Avg. loss: 0.300054\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014300, T: 20200, Avg. loss: 0.299188\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 101 epochs took 0.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_std.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.04249\n",
      "Feature: 1, Score: 0.02585\n",
      "Feature: 2, Score: 1.07273\n"
     ]
    }
   ],
   "source": [
    "#Negative feature importance value means that feature makes the loss go up.This might mean that your model is underfit (not enough iteration and it has not used the feature enough) or that the feature is not good and you can try removing it to improve final quality\n",
    "\n",
    "importance = clf_svm_std.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations From Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss at final epoch in SVM training is least compared to that of log loss in Logistic regression training final epoch.\n",
    "\n",
    "After standardizing data feature importance of feature 3 is highest and feature 2 importance is positive after SVM training"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "8B_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
